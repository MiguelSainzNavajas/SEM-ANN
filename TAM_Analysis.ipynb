{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc96631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEM Analysis\n",
    "\n",
    "import pandas as pd, plspm.config as c\n",
    "from plspm.plspm import Plspm\n",
    "from plspm.scheme import Scheme\n",
    "from plspm.mode import Mode\n",
    "\n",
    "TAM = pd.read_csv('TAM.csv', index_col=0)\n",
    "\n",
    "# Establish relationships between constructs\n",
    "structure = c.Structure()\n",
    "structure.add_path([\"SN\"], [\"PU\", \"PEU\", \"BI\", \"ATT\"])\n",
    "structure.add_path([\"OQ\"], [\"PU\"])\n",
    "structure.add_path([\"CP\"], [\"PEU\", \"ATT\", \"BI\"])\n",
    "structure.add_path([\"PEU\"], [\"PU\", \"ATT\", \"BI\", \"ASU\"])\n",
    "structure.add_path([\"PU\"], [\"ATT\", \"BI\"])\n",
    "structure.add_path([\"ATT\"], [\"BI\"])\n",
    "structure.add_path([\"BI\"], [\"ASU\"])\n",
    "\n",
    "# Establish relationships between manifest variables and constructs\n",
    "config = c.Config(structure.path(), scaled=False)\n",
    "config.add_lv_with_columns_named(\"OQ\", Mode.A, TAM, \"OQ\")\n",
    "config.add_lv_with_columns_named(\"SN\", Mode.A, TAM, \"SN\")\n",
    "config.add_lv_with_columns_named(\"PU\", Mode.A, TAM, \"PU\")\n",
    "config.add_lv_with_columns_named(\"ATT\", Mode.A, TAM, \"ATT\")\n",
    "config.add_lv_with_columns_named(\"BI\", Mode.A, TAM, \"BI\")\n",
    "config.add_lv_with_columns_named(\"ASU\", Mode.A, TAM, \"ASU\")\n",
    "config.add_lv_with_columns_named(\"CP\", Mode.A, TAM, \"CP\")\n",
    "config.add_lv_with_columns_named(\"PEU\", Mode.A, TAM, \"PEU\")\n",
    "\n",
    "# Check that the data are quantitative metric scale type\n",
    "print(config.metric())\n",
    "\n",
    "# Estimate the path model, using the Partial Least Squares (PLS) algorithm\n",
    "plspm_calc = Plspm(TAM, config, Scheme.PATH, bootstrap=True, bootstrap_iterations=5000)\n",
    "\n",
    "# Visualize results\n",
    "print(plspm_calc.goodness_of_fit())\n",
    "print(plspm_calc.unidimensionality())\n",
    "print(plspm_calc.inner_summary())\n",
    "print(plspm_calc.crossloadings())\n",
    "print(plspm_calc.outer_model())\n",
    "print(plspm_calc.inner_model())\n",
    "print(plspm_calc.path_coefficients())\n",
    "print(plspm_calc.effects())\n",
    "print(plspm_calc.scores())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1749cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Common Method Bias (CMB) test\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import f\n",
    "\n",
    "# Calculate the correlation matrix between the observed variables using the crossloadings\n",
    "df=plspm_calc.crossloadings()\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# Calculate the principal components of the correlation matrix\n",
    "eigenvalues, eigenvectors = np.linalg.eig(corr_matrix)\n",
    "first_component = eigenvectors[:, 0]\n",
    "\n",
    "# Calculate the loading of each manifest variable in the first principal component\n",
    "loadings = np.abs(df @ first_component)\n",
    "\n",
    "# Calculate the proportion of total variance explained by the first principal component\n",
    "total_variance = np.sum(eigenvalues)\n",
    "explained_variance = eigenvalues[0]\n",
    "explained_variance_ratio = explained_variance / total_variance\n",
    "\n",
    "print(f\"The first component explains {explained_variance_ratio * 100:.2f}% of the total variance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d1a1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimination of unsupported hypotheses\n",
    "\n",
    "import pandas as pd, plspm.config as c\n",
    "from plspm.plspm import Plspm\n",
    "from plspm.scheme import Scheme\n",
    "from plspm.mode import Mode\n",
    "\n",
    "TAM = pd.read_csv('TAM.csv', index_col=0)\n",
    "\n",
    "# Establish relationships between constructs\n",
    "structure = c.Structure()\n",
    "structure.add_path([\"SN\"], [\"PEU\", \"BI\"])\n",
    "structure.add_path([\"OQ\"], [\"PU\"])\n",
    "structure.add_path([\"CP\"], [\"PEU\", \"ATT\", \"BI\"])\n",
    "structure.add_path([\"PEU\"], [\"PU\", \"ATT\"])\n",
    "structure.add_path([\"PU\"], [\"ATT\"])\n",
    "structure.add_path([\"ATT\"], [\"BI\"])\n",
    "structure.add_path([\"BI\"], [\"ASU\"])\n",
    "\n",
    "# Establish relationships between manifest variables and constructs\n",
    "config = c.Config(structure.path(), scaled=False)\n",
    "config.add_lv_with_columns_named(\"OQ\", Mode.A, TAM, \"OQ\")\n",
    "config.add_lv_with_columns_named(\"SN\", Mode.A, TAM, \"SN\")\n",
    "config.add_lv_with_columns_named(\"PU\", Mode.A, TAM, \"PU\")\n",
    "config.add_lv_with_columns_named(\"ATT\", Mode.A, TAM, \"ATT\")\n",
    "config.add_lv_with_columns_named(\"BI\", Mode.A, TAM, \"BI\")\n",
    "config.add_lv_with_columns_named(\"ASU\", Mode.A, TAM, \"ASU\")\n",
    "config.add_lv_with_columns_named(\"CP\", Mode.A, TAM, \"CP\")\n",
    "config.add_lv_with_columns_named(\"PEU\", Mode.A, TAM, \"PEU\")\n",
    "\n",
    "# Estimate the path model, using the Partial Least Squares (PLS) algorithm\n",
    "plspm_calc = Plspm(TAM, config, Scheme.PATH, bootstrap=True, bootstrap_iterations=5000)\n",
    "\n",
    "# Visualize results\n",
    "print(plspm_calc.goodness_of_fit())\n",
    "print(plspm_calc.unidimensionality())\n",
    "print(plspm_calc.inner_summary())\n",
    "print(plspm_calc.crossloadings())\n",
    "print(plspm_calc.outer_model())\n",
    "print(plspm_calc.inner_model())\n",
    "print(plspm_calc.path_coefficients())\n",
    "print(plspm_calc.effects())\n",
    "print(plspm_calc.scores())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145b8268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset with the values of the constructs for each record (each surveyed person)\n",
    "plspm_calc.scores().to_csv('TAM_constructs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183258fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data in the range [0, 1]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read CSV file with original data\n",
    "df = pd.read_csv('TAM_constructs.csv')\n",
    "\n",
    "# Normalize the data in each column\n",
    "for col in df.columns:\n",
    "    X_min = df[col].min()\n",
    "    X_max = df[col].max()\n",
    "    df[col] = (df[col] - X_min) / (X_max - X_min)\n",
    "\n",
    "# Write CSV file with normalized data\n",
    "df.to_csv('TAM_normalized_constructs.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469496b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN Analysis: BI construct prediction and sensitivity analysis\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define custom RMSE loss function\n",
    "def rmse_loss(y_true, y_pred):\n",
    "    mse = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "    rmse = tf.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "# Load data from CSV file: columns of constructs not related to BI have been removed in a separate csv file\n",
    "data = pd.read_csv('TAM_normalized_constructs_BI_prediction.csv')\n",
    "\n",
    "# Split data into inputs (X) and output (y)\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1:].values\n",
    "\n",
    "# Define the number of folds for cross validation and the lists for storing the results of each fold\n",
    "n_folds = 10\n",
    "rmse_test_list = []\n",
    "rmse_train_list = []\n",
    "r2_test_list = []\n",
    "r2_train_list=[]\n",
    "importance_list = []\n",
    "\n",
    "# Create KFold instance\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate on each fold\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    \n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    \n",
    "    # Splitting data into training and test sets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Create the neural network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(2, input_dim=3, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=rmse_loss, optimizer=optimizers.RMSprop(learning_rate=0.001))\n",
    "\n",
    "    # Training of the model\n",
    "    history = model.fit(X_train, y_train, epochs=1000, batch_size=2, verbose=0, validation_data=(X_test, y_test))\n",
    "\n",
    "    # Obtain model predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "\n",
    "    # Evaluate the model in the train set\n",
    "    rmse_train = model.evaluate(X_train, y_train, verbose=0)\n",
    "    rmse_train_list.append(rmse_train)\n",
    "    print(f\"RMSE train: {rmse_train:.5f}\")\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    r2_train_list.append(r2_train)\n",
    "    print(f\"R2 train: {r2_train:.5f}\")\n",
    "    \n",
    "    # Evaluate the model in the test set\n",
    "    rmse_test = model.evaluate(X_test, y_test, verbose=0)\n",
    "    rmse_test_list.append(rmse_test)\n",
    "    print(f\"RMSE test: {rmse_test:.5f}\")\n",
    "    r2_test = r2_score(y_test, y_pred)\n",
    "    r2_test_list.append(r2_test)\n",
    "    print(f\"R2 test: {r2_test:.5f}\")\n",
    "\n",
    "    # Calculate the relative importance of each input predictor\n",
    "    layer = model.layers[0]\n",
    "    weights = layer.get_weights()\n",
    "    importance = np.abs(weights[0])\n",
    "    importance_avg = np.mean(importance, axis=1)\n",
    "    importance_list.append(importance_avg)\n",
    "    print()\n",
    "    print(\"Relative importance of each input predictor:\\n\", importance_avg)\n",
    "        \n",
    "    # Visualize the training\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Training progress')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the neural network\n",
    "plot_model(model, to_file='TAM_model.png', show_shapes=True, show_layer_names=True)\n",
    "print(model.summary())\n",
    "    \n",
    "# Calculate mean RMSE and standard deviation\n",
    "rmse_train_mean = np.mean(rmse_train_list)\n",
    "rmse_train_std = np.std(rmse_train_list)\n",
    "print(f\"RMSE train mean: {rmse_train_mean:.5f} +/- {rmse_train_std:.5f}\")\n",
    "rmse_test_mean = np.mean(rmse_test_list)\n",
    "rmse_test_std = np.std(rmse_test_list)\n",
    "print(f\"RMSE test mean: {rmse_test_mean:.5f} +/- {rmse_test_std:.5f}\")\n",
    "\n",
    "# Calculate mean R2\n",
    "r2_train_mean = np.mean(r2_train_list)\n",
    "print(f\"R2 train mean: {r2_train_mean:.5f}\")\n",
    "r2_test_mean = np.mean(r2_test_list)\n",
    "print(f\"R2 test mean: {r2_test_mean:.5f}\")\n",
    "\n",
    "# Display summary table and export it to csv\n",
    "print()\n",
    "results_df = pd.DataFrame({\"RMSE test\": rmse_test_list, \"R2 test\": r2_test_list,\"RMSE train\": rmse_train_list, \"R2 train\": r2_train_list})\n",
    "print(\"Results of each fold:\\n\", results_df)\n",
    "results_df.to_csv(\"Results_TAM.csv\", index=True)\n",
    "print()\n",
    "\n",
    "# Calculate the mean relative importance of each input predictor\n",
    "print()\n",
    "importance_df = pd.DataFrame(importance_list, columns=data.columns[:-1])\n",
    "importance_mean = importance_df.mean()\n",
    "\n",
    "# Display summary table of relative importance and export it to csv\n",
    "print(\"Relative importance of each input predictor for each fold:\\n\", importance_df)\n",
    "print()\n",
    "print(\"Mean relative importance of each input predictor:\")\n",
    "print(importance_mean)\n",
    "importance_df.to_csv(\"Sensitivity_analysis_TAM.csv\", index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
